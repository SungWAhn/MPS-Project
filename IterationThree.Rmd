---
title: "MPS Project -- Iteration 3"
author: "Sung-Woo Ahn Wanwen Gu Yvonne Liu Xinyue Chen"
date: "April 5, 2019"
output: 
  html_document:
    highlight: pygments
    theme: cosmo
---

<style type="text/css">

body{ /* Normal  */
      font-size: 18px;
  }

</style>



## Import dataset
```{r warning=FALSE, message=FALSE, error=FALSE}
library(knitr)
library(data.table)
library(bit64)
library(Rmpfr)
library(tidyverse)
library(kableExtra)
library(VIM)
library(DT)
library(e1071)
library(klaR)
library(mlbench)
library(corrplot)
library(bnlearn)
library(gridExtra)
library(plotly)
library(MLmetrics)
library(nnet)
library(randomForest)
library(caret)
```
```{r}
versions<-sessionInfo()
versions
```
```{r echo=FALSE}
options(kableExtra.latex.load_packages = FALSE)
knit_hooks$set(document = function(x) {
  sub('\\usepackage[]{color}', '\\usepackage[]{xcolor}', x, fixed = TRUE)
})

```


```{r}
train.final<-fread('training_final.csv', header=T, integer64='character')
valid.final<-fread('validation_final.csv',header=T, integer64='character')
train.NoNANoM<-fread('training_NoNA_NoMultc.csv', header = T, integer64 = 'character')
valid.NoNANoM<-fread('validation_NoNA_NoMultc.csv', header = T, integer64 = 'character')

names(train.NoNANoM)[24] <- "socialnetworking.appCt"
names(train.NoNANoM)[46] <- "socialnetworking.isactiveRat"
names(valid.NoNANoM)[24] <- "socialnetworking.appCt"
names(valid.NoNANoM)[46] <- "socialnetworking.isactiveRat"
```
```{r}
traintotaluncorr <- fread('traintotaluncorr.csv', header=T, integer64='character')
validtotaluncorr <- fread('validtotaluncorr.csv', header=T, integer64='character')
traintotaluncorr$group <- as.factor(traintotaluncorr$group)
validtotaluncorr$group <- as.factor(validtotaluncorr$group)
```



## Baseline -- All the features
Try fitting 4 different models: Naive Bayes, Random Forest, SVM, and Logistic model, on all the features we have so far.

### Naive Bayes
```{r}
traintotal <- cbind(train.NoNANoM[,-c(1,3,4,66,67)], train.final[,c(3,4)])
validtotal <- cbind(valid.NoNANoM[,-c(1,3,4,66,67)], valid.final[,c(3,4)])
traintotal$group <- as.factor(traintotal$group)
validtotal$group <- as.factor(validtotal$group)
```
```{r}
fitnb<-naiveBayes(group~., data=traintotal)
prednb<-predict(fitnb, validtotal)
prednbprobs<-predict(fitnb, validtotal, type='raw')
summary(prednb)
misclassnb <- mean(prednb!=validtotal$group)

#log-loss
loglossnb <- MultiLogLoss(y_pred = prednbprobs, y_true = validtotal$group)
```
```{r}
kable(data.frame('Misclassification'=c("91.77%"), 'logloss' = loglossnb), caption = 'Naive Bayes Performance: All features') %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "responsive"), full_width = F)
```


```{r}
confusion1 <- as.data.frame(table(prednb, validtotal$group))
p<-ggplot(confusion1, aes(Var2, prednb)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='All Features')
```

```{r}
bndat <- learning.test
bnlabeltrain <- labeltrain[,c(5,7)]
bnlabeltrain<-bnlabeltrain[which(is.na(bnlabeltrain$category)==FALSE),]
bnlabeltrain$category <- as.factor(bnlabeltrain$category)
bnlabeltrain$group <- as.factor(bnlabeltrain$group)
bnlabeltrain
bn <-  naive.bayes(bnlabeltrain, training="group",explanatory = 'category')
fit <- bn.fit(bn, bnlabeltrain)
fit
pred <- predict(bn, bnlabeltrain, prob=TRUE)
summary(pred)
pr <- attributes(pred)$prob
MultiLogLoss(y_pred = pr, y_true = validtotal$group)
```

### RF
```{r}
set.seed(1223)
fit.rf <- randomForest(as.factor(group)~., data = traintotal, na.action = na.omit)

rf.pred <- predict(fit.rf, newdata=validtotal)
table(rf.pred, validtotal$group)
mean(rf.pred[is.na(rf.pred)==FALSE]!=validtotal$group[is.na(rf.pred)==FALSE])

predrfprobs <-predict(fit.rf, validtotal, type = 'prob')
MultiLogLoss(y_pred = predrfprobs, y_true = validtotal$group)

ginis<-importance(fit.rf)
ginis<-data.frame('MeanDecreaseGini'=ginis[,1], 'Features' = rownames(ginis))
ginis<-arrange(ginis, desc(MeanDecreaseGini))
```

```{r results='asis'}
performancerfall <-data.frame('Misclassification' = c("79.60%"), 'Logloss' = c(2.264044))
kable(performancerfall, caption = "Random Forest Performance: All Count Predictors") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F)
```
```{r}
confusionrfall <- as.data.frame(table(rf.pred, validtotal$group))
prfall<-ggplot(confusionrfall, aes(Var2, rf.pred)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='All Features')
```

```{r results='asis'}
kable(as.matrix(ginis)[47:63,], caption = "Random Forest Gini Index: All Features") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F)
```

### RF without imbalance
```{r}
set.seed(1223)
fit.rf <- randomForest(as.factor(group)~., data = downSampledTrain, na.action = na.omit)

rf.pred <- predict(fit.rf, newdata=validtotal)
table(rf.pred, validtotal$group)
mean(rf.pred[is.na(rf.pred)==FALSE]!=validtotal$group[is.na(rf.pred)==FALSE])

predrfprobs <-predict(fit.rf, validtotal, type = 'prob')
MultiLogLoss(y_pred = predrfprobs, y_true = validtotal$group)
```

```{r results='asis'}
performancerfallup <-data.frame('Misclassification' = c("81.09%"), 'Logloss' = c(2.281839))
kable(performancerfall, caption = "Random Forest Performance: All Features & Upsampled") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F)
```
```{r}
confusionrfallup <- as.data.frame(table(rf.pred, validtotal$group))
prfallup<-ggplot(confusionrfallup, aes(Var2, rf.pred)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='All Features')
```

```{r results='asis'}
performancerfalldown <-data.frame('Misclassification' = c("83.69%"), 'Logloss' = c(2.355479))
kable(performancerfdown, caption = "Random Forest Performance: All Features & Downsampled") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F)
```
```{r}
confusionrfalldown <- as.data.frame(prop.table(table(rf.pred, validtotal$group),2))
prfalldown<-ggplot(confusionrfalldown, aes(Var2, rf.pred)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='All Features & Downsampled')
```


### SVM
```{r}
# Scale all features
test<-rbind(traintotal,validtotal)
new<-scale(test[,-1])
new1<-data.frame("group"=test$group,new)
traintotalsc <- new1[1:18594,]
validtotalsc <- new1[18595:23247,]
```
```{r}
set.seed(1)
sub<-sample.int(n=nrow(traintotal), size = floor(.4*nrow(traintotal)), replace = F)
tuned<-tune.svm(group ~ ., data = traintotal[sub,], gamma = 10^(-5:3), cost = 10^(-3:2))
saveRDS(tuned,file="tunesvmtotal.rds")
```
```{r}
fit.svm <- svm(as.factor(group) ~ . , traintotal, gamma=.0002442406, cost=4096, probability=TRUE)

fit.svm <- svm(as.factor(group) ~ . , traintotal, probability=TRUE)
predsvm <- predict(fit.svm, validtotal, probability = T)
mean(as.character(predsvm) != as.character(validtotal$group))
table(predsvm, validtotal$group)

predsvmprobs <- attr(predsvm, 'probabilities')
MultiLogLoss(y_pred = predsvmprobs, y_true = validtotal$group)
```

```{r results='asis'}
performancesvmall <-data.frame('Misclassification' = c("81.19%"), 'Logloss' = c(2.655779))
kable(performancesvmall, caption = "SVM Performance: All Count Predictors") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F)
```
```{r}
confusionsvmall <- as.data.frame(table(predsvm, validtotal$group))
psvmall<-ggplot(confusionsvmall, aes(Var2, predsvm)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='All Features')
```



### Logistic
```{r}
train_log<- traintotal
train_log<-na.omit(train_log)
valid_log<-validtotal
valid_log<-na.omit(valid_log)

model_log <- multinom(group ~ ., data = traintotal, MaxNWts = 100000, model = TRUE, probability = TRUE)
pred_log<-predict(model_log,valid_log)
pred_log_score<-predict (model_log,valid_log, 'probs')
mean(as.character(pred_log) != as.character(valid_log$group))
table(pred_log, valid_log$group)
# coef(model_log)

predlogprobsdat <- data.frame(validtotal$group, pred_log_score)
logloss <- vector(length = length(predlogprobsdat$validtotal.group))
for (i in 1:length(predlogprobsdat$validtotal.group)) {
  clas=grep(str_split(paste(predlogprobsdat$validtotal.group[i]), '-')[[1]][1], names(predlogprobsdat))
  logloss[i] = log(predlogprobsdat[i,clas])
}
-mean(logloss)
```


```{r}
train_log<- traintotaluncorr
train_log<-na.omit(train_log)
valid_log<-validtotaluncorr
valid_log<-na.omit(valid_log)

model_log <- multinom(group ~ ., data = traintotaluncorr, MaxNWts = 100000, model = TRUE, probability = TRUE)
pred_log<-predict(model_log,valid_log)
pred_log_score<-predict (model_log,valid_log, 'probs')
mean(as.character(pred_log) != as.character(valid_log$group))
table(pred_log, valid_log$group)

predlogprobsdat <- data.frame(validtotaluncorr$group, pred_log_score)
logloss <- vector(length = length(predlogprobsdat$validtotaluncorr.group))
for (i in 1:length(predlogprobsdat$validtotaluncorr.group)) {
  clas=grep(str_split(paste(predlogprobsdat$validtotaluncorr.group[i]), '-')[[1]][1], names(predlogprobsdat))
  logloss[i] = log(predlogprobsdat[i,clas])
}
-mean(logloss)
```

```{r results='asis'}
performancelogred <-data.frame('Misclassification' = c("83.22%"), 'Logloss' = c(2.398638))
kable(performancelogred, caption = "Multinomial Logistic Regression Performance:\n Reduced Feature Set") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F)
```
```{r}
confusionlogred <- as.data.frame(table(pred_log, validtotaluncorr$group))
plogred<-ggplot(confusionlogred, aes(Var2, pred_log)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='Reduced Feature Set')
```

# upsampled
```{r}
train_log<- upSampledTrain
train_log<-na.omit(train_log)
valid_log<-validtotaluncorr
valid_log<-na.omit(valid_log)

model_log <- multinom(group ~ ., data = upSampledTrain, MaxNWts = 100000, model = TRUE, probability = TRUE)
pred_log<-predict(model_log,valid_log)
pred_log_score<-predict (model_log,valid_log, 'probs')
mean(as.character(pred_log) != as.character(valid_log$group))
table(pred_log, valid_log$group)

predlogprobsdat <- data.frame(validtotaluncorr$group, pred_log_score)
logloss <- vector(length = length(predlogprobsdat$validtotaluncorr.group))
for (i in 1:length(predlogprobsdat$validtotaluncorr.group)) {
  clas=grep(str_split(paste(predlogprobsdat$validtotaluncorr.group[i]), '-')[[1]][1], names(predlogprobsdat))
  logloss[i] = log(predlogprobsdat[i,clas])
}
-mean(logloss)
```

```{r results='asis'}
performancelogredeq <-data.frame('Misclassification' = c("87.75%"), 'Logloss' = c(2.470736))
kable(performancelogredeq, caption = "Multinomial Logistic Regression Performance:\n Reduced Feature Set and Equal Class size") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F)
```
```{r}
confusionlogredeq <- as.data.frame(table(pred_log, validtotaluncorr$group))
plogredeq<-ggplot(confusionlogredeq, aes(Var2, pred_log)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='Reduced Feature Set')
```


### Performance
Predictably, Naive Bayes is worst, with Random Forest having the best misclassification and logloss.


## Naive Bayes with Kernel

```{r warning=FALSE, message=FALSE, error=FALSE}
fitnb<-NaiveBayes(group~., data=traintotal, usekernel=TRUE)
prednb<-predict(fitnb, validtotal, threshold = .0001)
summary(prednb)
mean(prednb$class!=validtotal$group)
which(is.nan(prednb$posterior[,1]))
#log-loss
MultiLogLoss(y_pred = prednb$posterior[-3827,], y_true = validtotal$group[-3827])
```

```{r warning=FALSE, message=FALSE, error=FALSE}
fitnb<-NaiveBayes(group~., data=traintotaluncorr, usekernel=TRUE)
prednb<-predict(fitnb, validtotaluncorr, threshold = .0001)
summary(prednb)
mean(prednb$class!=validtotaluncorr$group)
which(is.nan(prednb$posterior[,1]))
#log-loss
MultiLogLoss(y_pred = prednb$posterior[-3827,], y_true = validtotaluncorr$group[-3827])
```

```{r results='asis'}
performancebase <-data.frame('Misclass' = c("88.48%"), 'Logloss' = c(13.66825))
kable(performancebase) %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F) %>%
  column_spec(1, width = '7em') %>%
  column_spec(2, width = "7em") %>% 
  footnote(general = "Naive Bayes Performance:\n All Count Predictors\n & Flexible Distribution", footnote_as_chunk = T, title_format = c("italic", "underline"), threeparttable = T
           )
```
```{r results='asis'}
performancebase <-data.frame('Misclass' = c("88.27%"), 'Logloss' = c(9.454497))
kable(performancebase) %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F) %>%
  column_spec(1, width = '7em') %>%
  column_spec(2, width = "7em") %>% 
  footnote(general = "Naive Bayes Performance:\n Reduced Count Predictors\n & Flexible Distribution", footnote_as_chunk = T, title_format = c("italic", "underline"), threeparttable = T
           )
```
```{r}
confusion <- as.data.frame(table(prednb$class, validtotal$group))
p<-ggplot(confusion, aes(Var2, Var1)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='All features')
```


### Counts as fine grained categorical
```{r}
countrange <- vector(length = 24)
traintotalframe <- as.data.frame(traintotal)
for (i in 2:25) {
  countrange[i-1] <- length(unique(traintotalframe[,i]))
}
names(countrange)<-names(traintotal[,2:25])
countrange[order(countrange)]

traintotal2<-as.data.frame(traintotal)
validtotal2 <- as.data.frame(validtotal)
all2 <- as.data.frame(rbind(traintotal2,validtotal2))

for (i in 2:25) {
  varLevels <- sort(unique(all2[,i]))
  traintotal2[, i] <- factor(traintotal2[,i], levels = paste(varLevels))
  validtotal2[, i] <- factor(validtotal2[,i],levels = paste(varLevels))
}

traintotal2$sport.appCt <- as.factor(traintotal2$sport.appCt)
traintotal2$food.appCt <- as.factor(traintotal2$food.appCt)
traintotal2$household.appCt <- as.factor(traintotal2$household.appCt)
traintotal2$estate.appCt <- as.factor(traintotal2$estate.appCt)
traintotal2$count.app <- as.factor(traintotal2$count.app)

validtotal2$sport.appCt <- as.factor(validtotal2$sport.appCt)
validtotal2$food.appCt <- as.factor(validtotal2$food.appCt)
validtotal2$household.appCt <- as.factor(validtotal2$household.appCt)
validtotal2$estate.appCt <- as.factor(validtotal2$estate.appCt)
validtotal2$count.app <- as.factor(validtotal2$count.app)
```

```{r}
fitnb<-naiveBayes(group~., data=traintotal2)
prednb<-predict(fitnb, validtotal2)
prednbprobs<-predict(fitnb, validtotal2, type='raw')
summary(prednb)
mean(prednb!=validtotal2$group)

#log-loss
MultiLogLoss(y_pred = prednbprobs, y_true = validtotal2$group)
```
```{r warning=FALSE, message=FALSE, error=FALSE}
fitnb<-NaiveBayes(group~., data=traintotal2, usekernel=TRUE)
prednb<-predict(fitnb, validtotal2, threshold = .0001)
summary(prednb)
mean(prednb$class!=validtotal2$group)
which(is.nan(prednb$posterior[,1]))
#log-loss
MultiLogLoss(y_pred = prednb$posterior[-3827,], y_true = validtotal2$group[-3827])
```



## Try creating categorical features
Note: We used the percentiles as guidelines when binning, but sometimes we found that changing the boundaries made the performance of the models better.

### Event count and app count
```{r}
# Obtain the app count features we want to bin
trainingctEvent <- as.data.frame(train.NoNANoM[,5])
# Stack the app count features into one long vector
counts <- stack(trainingctEvent)
# Calculate the quantiles
quantiles <- quantile(counts[,1], c(.25,.5,.75))
quantiles

# Construct the categorical features based on the quantiles
trainingctEvent[trainingctEvent==0] <- 0
trainingctEvent[trainingctEvent>0 & trainingctEvent <= 4] <- 1
trainingctEvent[trainingctEvent>4 & trainingctEvent <= 15] <- 2
trainingctEvent[trainingctEvent>15 & trainingctEvent <= 51] <- 3
trainingctEvent[trainingctEvent>51] <- 4


# Convert the categorical into factor variables
trainingctEvent$count_event<- as.factor(trainingctEvent$count_event) 
# The levels are not consistent across features: make them all 5
trainingctEvent$count_event <- factor(trainingctEvent$count_event, levels = c("0","1","2","3","4"))
names(trainingctEvent) <- paste0(names(trainingctEvent),'cat')

# Do the same thing as above to the validation dataset
validationctEvent <- as.data.frame(valid.NoNANoM[,5])



validationctEvent[validationctEvent==0] <- 0
validationctEvent[validationctEvent>0 & validationctEvent <= 4] <- 1
validationctEvent[validationctEvent>4 & validationctEvent <= 15] <- 2
validationctEvent[validationctEvent>15 & validationctEvent <= 51] <- 3
validationctEvent[validationctEvent>51] <- 4


validationctEvent$count_event<- as.factor(validationctEvent$count_event)
# The levels are not consistent across features: make them all 5
validationctEvent$count_event <- factor(validationctEvent$count_event, levels = c("0","1","2","3","4"))
names(validationctEvent) <- paste0(names(validationctEvent),'cat')
```
```{r}
# Obtain the app count features we want to bin
trainingctApp <- as.data.frame(train.NoNANoM[,6])
# Stack the app count features into one long vector
counts <- stack(trainingctApp)
# Calculate the quantiles
quantiles <- quantile(counts[,1], c(.25,.5,.75))
quantiles

# Construct the categorical features based on the quantiles
trainingctApp[trainingctApp==0] <- 0
trainingctApp[trainingctApp>0 & trainingctApp <= 24] <- 1
trainingctApp[trainingctApp>24 & trainingctApp <= 35] <- 2
trainingctApp[trainingctApp>35 & trainingctApp <= 50] <- 3
trainingctApp[trainingctApp>50] <- 4


# Convert the categorical into factor variables
trainingctApp$count.app<- as.factor(trainingctApp$count.app) 
# The levels are not consistent across features: make them all 5
trainingctApp$count.app <- factor(trainingctApp$count.app, levels = c("0","1","2","3"))
names(trainingctApp) <- paste0(names(trainingctApp),'cat')

# Do the same thing as above to the validation dataset
validationctApp <- as.data.frame(valid.NoNANoM[,6])



validationctApp[validationctApp==0] <- 0
validationctApp[validationctApp>0 & validationctApp <= 24] <- 1
validationctApp[validationctApp>24 & validationctApp <= 35] <- 2
validationctApp[validationctApp>35 & validationctApp <= 50] <- 3
validationctApp[validationctApp>50] <- 4


validationctApp$count.app<- as.factor(validationctApp$count.app) 
# The levels are not consistent across features: make them all 5
validationctApp$count.app <- factor(validationctApp$count.app, levels = c("0","1","2","3"))
names(validationctApp) <- paste0(names(validationctApp),'cat')
```



### App Counts
```{r}
# Obtain the app count features we want to bin
trainingAppct <- as.data.frame(train.NoNANoM[,7:29])
# Stack the app count features into one long vector
counts <- stack(trainingAppct)
# Calculate the quantiles
quantiles <- quantile(counts[,1], c(.25,.5,.75))
quantiles

# Construct the categorical features based on the quantiles
trainingAppct[trainingAppct==0] <- 0
trainingAppct[trainingAppct>0 & trainingAppct <= 5] <- 1
trainingAppct[trainingAppct>5 & trainingAppct <= 35] <- 2
trainingAppct[trainingAppct>35] <- 3


# Convert the categorical into factor variables
for (i in 1:23) {
 trainingAppct[,i] <- as.factor(trainingAppct[,i]) 
}
for (i in 1:23) {
  names(trainingAppct)[i] <- paste0(names(trainingAppct)[i],'cat')
}

# The levels are not consistent across features: make them all 5
for (i in 1:23) {
  trainingAppct[,i] <- factor(trainingAppct[,i], levels = c("0","1","2","3"))
}

# Do the same thing as above to the validation dataset
validationAppct <- as.data.frame(valid.NoNANoM[,7:29])



validationAppct[validationAppct==0] <- 0
validationAppct[validationAppct>0 & validationAppct <= 5] <- 1
validationAppct[validationAppct>5 & validationAppct <= 35] <- 2
validationAppct[validationAppct>35] <- 3


for (i in 1:23) {
 validationAppct[,i] <- as.factor(validationAppct[,i]) 
}
for (i in 1:23) {
  names(validationAppct)[i] <- paste0(names(validationAppct)[i],'cat')
}

for (i in 1:23) {
  validationAppct[,i] <- factor(validationAppct[,i], levels = c("0","1","2","3"))
}
```

### Is_active Ratio
```{r}
# Obtain the app count features we want to bin
trainingIsactive <- as.data.frame(train.NoNANoM[,30:51])
# Stack the app count features into one long vector
counts <- stack(trainingIsactive)
# Calculate the quantiles
quantiles <- quantile(counts[,1], c(.25,.5,.75))
quantiles

# Construct the categorical features based on the quantiles
trainingIsactive[trainingIsactive==0] <- 0
trainingIsactive[trainingIsactive>0 & trainingIsactive <= 3] <- 1
trainingIsactive[trainingIsactive>3] <- 2

# Convert the categorical into factor variables
for (i in 1:22) {
 trainingIsactive[,i] <- as.factor(trainingIsactive[,i]) 
}
for (i in 1:22) {
  names(trainingIsactive)[i] <- paste0(names(trainingIsactive)[i],'cat')
}

# The levels are not consistent across features: make them all 5
for (i in 1:22) {
  trainingIsactive[,i] <- factor(trainingIsactive[,i], levels = c("0","1","2"))
}

# Do the same thing as above to the validation dataset
validIsactive <- as.data.frame(valid.NoNANoM[,30:51])



validIsactive[validIsactive==0] <- 0
validIsactive[validIsactive>0 & validIsactive <= 3] <- 1
validIsactive[validIsactive>3] <- 2

for (i in 1:22) {
 validIsactive[,i] <- as.factor(validIsactive[,i]) 
}
for (i in 1:22) {
  names(validIsactive)[i] <- paste0(names(validIsactive)[i],'cat')
}

for (i in 1:22) {
  validIsactive[,i] <- factor(validIsactive[,i], levels = c("0","1","2"))
}

```

### Weekday
```{r}
# Obtain the app count features we want to bin
trainingWeekday <- as.data.frame(train.NoNANoM[,60])
# Stack the app count features into one long vector
counts <- stack(trainingWeekday)
# Calculate the quantiles
quantiles <- quantile(counts[,1], c(.25,.5,.75))
quantiles

# Construct the categorical features based on the quantiles
trainingWeekday[trainingWeekday==0] <- 0
trainingWeekday[trainingWeekday>0 & trainingWeekday <= 3] <- 1
trainingWeekday[trainingWeekday>3 & trainingWeekday <= 10] <- 2
trainingWeekday[trainingWeekday>10 & trainingWeekday <= 37] <- 3
trainingWeekday[trainingWeekday>37] <- 4


for (i in 1) {
 trainingWeekday[,i] <- as.factor(trainingWeekday[,i]) 
}
for (i in 1) {
  names(trainingWeekday)[i] <- paste0(names(trainingWeekday)[i],'cat')
}

# The levels are not consistent across features: make them all 5
for (i in 1) {
  trainingWeekday[,i] <- factor(trainingWeekday[,i], levels = c("0","1","2","3","4"))
}

# Do the same thing as above to the validation dataset
validWeekday <- as.data.frame(valid.NoNANoM[,60])



validWeekday[validWeekday==0] <- 0
validWeekday[validWeekday>0 & validWeekday <= 3] <- 1
validWeekday[validWeekday>3 & validWeekday <= 10] <- 2
validWeekday[validWeekday>10 & validWeekday <= 37] <- 3
validWeekday[validWeekday>37] <- 4


for (i in 1) {
 validWeekday[,i] <- as.factor(validWeekday[,i]) 
}
for (i in 1) {
  names(validWeekday)[i] <- paste0(names(validWeekday)[i],'cat')
}

for (i in 1) {
  validWeekday[,i] <- factor(validWeekday[,i], levels = c("0","1","2","3","4"))
}

```
### Hourly Counts
```{r}
trainCount48<-data.matrix(train.NoNANoM[, 'count.4_8'])
trainCount812<-data.matrix(train.NoNANoM[, 'count.8_12'])
trainCount1216<-data.matrix(train.NoNANoM[, 'count.12_16'])
trainCount1620<-data.matrix(train.NoNANoM[, 'count.16_20'])
trainCount2024<-data.matrix(train.NoNANoM[, 'count.20_24'])
hourcount<-rbind(trainCount48, trainCount812, trainCount1216, trainCount1620, trainCount2024) # 0 1 2
quantile<-quantile(hourcount, c(0.25, 0.5, 0.75))
quantile

trainCount48[trainCount48==0]<-0
trainCount48[trainCount48>0 & trainCount48<2]<-1
trainCount48[trainCount48>=2 & trainCount48<8]<-2
trainCount48[trainCount48>=8]<-3

trainCount812[trainCount812==0]<-0
trainCount812[trainCount812>0 & trainCount812<2]<-1
trainCount812[trainCount812>=2 & trainCount812<8]<-2
trainCount812[trainCount812>=8]<-3

trainCount1216[trainCount1216==0 ]<-0
trainCount1216[trainCount1216>0 & trainCount1216<2]<-1
trainCount1216[trainCount1216>=2 & trainCount1216<8]<-2
trainCount1216[trainCount1216>=8]<-3


trainCount1620[trainCount1620==0]<-0
trainCount1620[trainCount1620>0 & trainCount1620<2]<-1
trainCount1620[trainCount1620>=2 & trainCount1620<8]<-2
trainCount1620[trainCount1620>=8]<-3


trainCount2024[trainCount2024==0]<-0
trainCount2024[trainCount2024>0 & trainCount2024<2]<-1
trainCount2024[trainCount2024>=2 & trainCount2024<8]<-2
trainCount2024[trainCount2024>=8]<-3

traincount <- cbind(trainCount48,trainCount812,trainCount1216,trainCount1620,trainCount2024)
traincount <-as.data.frame(traincount)
for (i in 1:5) {
 traincount[,i] <- as.factor(traincount[,i])
 names(traincount)[i] <- paste0(names(traincount)[i],'cat')
 # The levels are not consistent across features: make them all 4
 traincount[,i] <- factor(traincount[,i], levels = c("0","1","2","3"))
}

validCount48<-data.matrix(valid.NoNANoM[, 'count.4_8'])
validCount812<-data.matrix(valid.NoNANoM[, 'count.8_12'])
validCount1216<-data.matrix(valid.NoNANoM[, 'count.12_16'])
validCount1620<-data.matrix(valid.NoNANoM[, 'count.16_20'])
validCount2024<-data.matrix(valid.NoNANoM[, 'count.20_24'])



validCount48[validCount48==0]<-0
validCount48[validCount48>0 & validCount48<2]<-1
validCount48[validCount48>=2 & validCount48<8]<-2
validCount48[validCount48>=8]<-3

validCount812[validCount812==0]<-0
validCount812[validCount812>0 & validCount812<2]<-1
validCount812[validCount812>=2 & validCount812<8]<-2
validCount812[validCount812>=8]<-3

validCount1216[validCount1216==0 ]<-0
validCount1216[validCount1216>0 & validCount1216<2]<-1
validCount1216[validCount1216>=2 & validCount1216<8]<-2
validCount1216[validCount1216>=8]<-3


validCount1620[validCount1620==0]<-0
validCount1620[validCount1620>0 & validCount1620<2]<-1
validCount1620[validCount1620>=2 & validCount1620<8]<-2
validCount1620[validCount1620>=8]<-3


validCount2024[validCount2024==0]<-0
validCount2024[validCount2024>0 & validCount2024<2]<-1
validCount2024[validCount2024>=2 & validCount2024<8]<-2
validCount2024[validCount2024>=8]<-3
validcount <- cbind(validCount48,validCount812,validCount1216,validCount1620,validCount2024)
validcount<-as.data.frame(validcount)

for (i in 1:5) {
 validcount[,i] <- as.factor(validcount[,i]) 
 names(validcount)[i] <- paste0(names(validcount)[i],'cat')
 # The levels are not consistent across features: make them all 4
 validcount[,i] <- factor(validcount[,i], levels = c("0","1","2","3"))
}

```

### Day Count
```{r}
# Obtain the day count features we want to bin
trainingDay <- as.data.frame(train.NoNANoM[,52:59])
# Stack the day count features into one long vector
counts <- stack(trainingDay)
# Calculate the quantiles
quantiles <- quantile(counts[,1], c(.25,.5,.75))
quantiles

# Construct the categorical features based on the quantiles
trainingDay[trainingDay==0] <- 0
trainingDay[trainingDay>0 & trainingDay <= 1] <- 1
trainingDay[trainingDay>1 & trainingDay <= 5] <- 2
trainingDay[trainingDay>5] <- 3

# Convert the categorical into factor variables
for (i in 1:8) {
 trainingDay[,i] <- as.factor(trainingDay[,i]) 
 names(trainingDay)[i] <- paste0(names(trainingDay)[i],'cat')
 # The levels are not consistent across features: make them all 4
 trainingDay[,i] <- factor(trainingDay[,i], levels = c("0","1","2","3"))
}


# Do the same thing as above to the validation dataset
validationDay <- as.data.frame(valid.NoNANoM[,52:59])

validationDay[validationDay==0] <- 0
validationDay[validationDay>0 & validationDay <= 1] <- 1
validationDay[validationDay>1 & validationDay <= 5] <- 2
validationDay[validationDay>5] <- 3

for (i in 1:8) {
 validationDay[,i] <- as.factor(validationDay[,i]) 
 names(validationDay)[i] <- paste0(names(validationDay)[i],'cat')
 # The levels are not consistent across features: make them all 4
 validationDay[,i] <- factor(validationDay[,i], levels = c("0","1","2","3"))
}

# fit the model with transformed categorical features
train.daycat <- trainingDay
valid.daycat <- validationDay
```

## Fit model with categorical predictors
```{r}
traincateg<-cbind(trainingctApp, trainingctEvent, trainingAppct, trainingIsactive, trainingWeekday, traincount, train.daycat)
traincateg$longitude<- traintotal$longitude
traincateg$latitude <- traintotal$latitude
traincateg$group <- traintotal$group
validcateg<-cbind(validationctApp, validationctEvent, validationAppct, validIsactive, validWeekday, validcount, valid.daycat)
validcateg$longitude<- validtotal$longitude
validcateg$latitude <- validtotal$latitude
validcateg$group <- validtotal$group
```

```{r}
highcorrcat <- paste0(highcorr,'cat')
traincateguncorr <- select(traincateg, -c(highcorrcat[1:length(highcorrcat)]))
validcateguncorr <- select(validcateg, -c(highcorrcat[1:length(highcorrcat)]))
```


### Naive Bayes
```{r}
fitnb<-naiveBayes(group~., data=traincateg)
prednb<-predict(fitnb, validcateg)
prednbprobs<-predict(fitnb, validcateg, type='raw')
summary(prednb)
mean(prednb!=validcateg$group)

#log-loss
MultiLogLoss(y_pred = prednbprobs, y_true = validcateg$group)
```
```{r}
fitnb<-naiveBayes(group~., data=traincateguncorr)
prednb<-predict(fitnb, validcateguncorr)
prednbprobs<-predict(fitnb, validcateguncorr, type='raw')
summary(prednb)
mean(prednb!=validcateguncorr$group)

#log-loss
MultiLogLoss(y_pred = prednbprobs, y_true = validcateguncorr$group)
```

```{r results='asis'}
performancecategall <-data.frame('Misclassification' = c("85.26%"), 'Logloss' = c(4.263261))
kable(performancecategall, caption = "Naive Bayes Performance: All Categorical Predictors") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F) %>%
  column_spec(1, width = '7em') %>%
  column_spec(2, width = "7em")
```

```{r results='asis'}
performancecategred <-data.frame('Misclassification' = c("84.01%"), 'Logloss' = c(2.480219))
kable(performancecategred, caption = "Naive Bayes Performance: Reduced Categorical Predictors") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F) %>%
  column_spec(1, width = '8em') %>%
  column_spec(2, width = "8em")
```
```{r}
confusion <- as.data.frame(table(prednb, validcateg$group))
pcategall<-ggplot(confusion, aes(Var2, prednb)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='All Categorical Features')
```
```{r}
confusion <- as.data.frame(table(prednb, validcateguncorr$group))
pcategred<-ggplot(confusion, aes(Var2, prednb)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='Reduced Categorical Features')
```


```{r}
downSampledcategTrain <- downSample(x = traincateguncorr[,-23],
                           y = traincateguncorr$group,
                           yname = "group")
upSampledcategTrain <- upSample(x = traincateguncorr[,-23], y = traincateguncorr$group,
                           yname = 'group')
```

```{r warning=FALSE, message=FALSE, error=FALSE}
fitnbupcateg<-naiveBayes(group~., data=upSampledcategTrain)
prednbupcateg<-predict(fitnbupcateg, validcateguncorr)
prednbupcategprobs<-predict(fitnbupcateg, validcateguncorr, type='raw')
summary(prednbupcateg)
mean(prednbupcateg!=validcateguncorr$group)

#log-loss
MultiLogLoss(y_pred = prednbupcategprobs, y_true = validcateguncorr$group)
```

```{r warning=FALSE, message=FALSE, error=FALSE}
fitnbdowncateg<-naiveBayes(group~., data=downSampledcategTrain)
prednbdowncateg<-predict(fitnbdowncateg, validcateguncorr)
prednbdowncategprobs<-predict(fitnbdowncateg, validcateguncorr, type='raw')
summary(prednbdowncateg)
mean(prednbdowncateg!=validcateguncorr$group)

#log-loss
MultiLogLoss(y_pred = prednbdowncategprobs, y_true = validcateguncorr$group)
```

```{r}
confusionup <- as.data.frame(table(prednbupcateg, validcateguncorr$group))
pup<-ggplot(confusionup, aes(Var2, prednbupcateg)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='Reduced Categorical Features, Upsampled')
```

```{r}
confusiondown <- as.data.frame(table(prednbdowncateg, validcateguncorr$group))
pdown<-ggplot(confusiondown, aes(Var2, prednbdowncateg)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='Reduced Categorical Features, Downsampled')
```

```{r results='asis'}
performanceup <-data.frame('Misclassification' = c("89.15%"), 'Logloss' = c(2.581781))
kable(performanceup) %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F) %>%
  column_spec(1, width = '7em') %>%
  column_spec(2, width = "7em") %>% 
  footnote(general = "Naive Bayes Performance:\n Reduced Categorical Predictors & upsampled", footnote_as_chunk = T, title_format = c("italic", "underline"), threeparttable = T
           )
```

```{r results='asis'}
performancedown <-data.frame('Misclassification' = c("87.41%"), 'Logloss' = c(2.53609))
kable(performancedown) %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F) %>%
  column_spec(1, width = '7em') %>%
  column_spec(2, width = "7em") %>% 
  footnote(general = "Naive Bayes Performance:\n Reduced Categorical Predictors & downsampled", footnote_as_chunk = T, title_format = c("italic", "underline"), threeparttable = T
           )
```




```{r warning=FALSE,message=FALSE}
fit.nb = list()
pred.nb = list()
pred.nbprobs = list()
logloss = vector(length=5)
misclass<-vector(length=5)

# traincumulative = matrix(nrow = dim(train.NoNANoM)[1])
# validcumulative = matrix(nrow = dim(valid.NoNANoM)[1])
traincumulative = matrix(nrow=18594)
validcumulative = matrix(nrow=4653)

for (i in 1:5) {
  traincumulative <- cbind(traincumulative, traincateg[[i]])
  validcumulative <- cbind(validcumulative, validcateg[[i]])
  fit.nb[[i]]<-naiveBayes(group~., data=traincumulative[,-1], usekernel=TRUE)
  pred.nb[[i]]<-predict(fit.nb[[i]], validcumulative[,-1], threshold = .0001)
  print(summary(pred.nb[[i]]))
  misclass[i]=mean(pred.nb[[i]]!=validcumulative$group)
  
  #log-loss
  pred.nbprobs[[i]] <-predict(fit.nb[[i]], validcumulative[,-1], type = 'raw')
  logloss[i]=MultiLogLoss(y_pred = pred.nbprobs[[i]], y_true = validcumulative$group) 
  
}
logloss
misclass
```

### RF
```{r}
set.seed(1223)
fit.rf = list()
pred.rf = list()
pred.rfprobs = list()
logloss = vector(length=5)
misclass<-vector(length=5)
traincumulative = matrix(nrow=18594)
validcumulative = matrix(nrow=4653)

for (i in 1:5) {
  traincumulative <- cbind(traincumulative, traincateg[[i]])
  validcumulative <- cbind(validcumulative, validcateg[[i]])
  fit.rf[[i]] <- randomForest(as.factor(group)~., data = traincumulative[,-1], na.action = na.omit)
  pred.rf[[i]] <- predict(fit.rf[[i]], newdata=validcumulative[,-1])
  print(summary(pred.rf[[i]]))
  misclass[i]=mean(pred.rf[[i]]!=validcumulative$group)
  
  #log-loss
  pred.rfprobs[[i]] <-predict(fit.rf[[i]], validcumulative[,-1], type = 'prob')
  logloss[i]=MultiLogLoss(y_pred = pred.rfprobs[[i]], y_true = validcumulative$group) 
  
}
misclass
logloss
```
```{r eval=FALSE,echo=FALSE}

table(rf.pred, validcateg$group)
mean(rf.pred[is.na(rf.pred)==FALSE]!=validcateg$group[is.na(rf.pred)==FALSE])




ginis<-importance(fit.rf)
ginis<-data.frame('gini'=ginis[,1], 'categ' = rownames(ginis))
arrange(ginis, desc(gini))
```

### SVM
```{r eval=FALSE}
fit.svm = list()
pred.svm = list()
pred.svmprobs = list()
logloss = vector(length=5)
misclass<-vector(length=5)
traincumulative = matrix(nrow=18594)
validcumulative = matrix(nrow=4653)

for (i in 1:5) {
  traincumulative <- cbind(traincumulative, traincateg[[i]])
  validcumulative <- cbind(validcumulative, validcateg[[i]])
  fit.svm[[i]] <- svm(as.factor(group) ~ . , traincumulative[,-1], probability=TRUE)
  pred.svm[[i]] <- predict(fit.svm[[i]], newdata=validcumulative[,-1])
  print(summary(pred.svm[[i]]))
  misclass[i]=mean(pred.svm[[i]]!=validcumulative$group)
  
  #log-loss
  pred.svmprobs[[i]] <-predict(fit.svm[[i]], validcumulative[,-1], probability = T)
  pred.svmprobs[[i]] <- attr(pred.svmprobs[[i]], 'probabilities')
  logloss[i]=MultiLogLoss(y_pred = pred.svmprobs[[i]], y_true = validcumulative$group) 
  
}
```
```{r eval=FALSE,echo=FALSE}
mean(as.character(pred_svm) != as.character(validcateg$group))
table(pred_svm, validcateg$group)
```

```{r}
load('categsvm')
misclass
logloss
```

### Logistic
```{r}
fit.log = list()
pred.log = list()
pred.logprobs = list()
logloss = vector(length=5)
misclass<-vector(length=5)
traincumulative = matrix(nrow=18594)
validcumulative = matrix(nrow=4653)

for (i in 1:5) {
  traincumulative <- cbind(traincumulative, traincateg[[i]])
  validcumulative <- cbind(validcumulative, validcateg[[i]])
  fit.log[[i]] <-  multinom(group ~ ., data = traincumulative[,-1], MaxNWts = 100000, model = TRUE, probability = TRUE)
  pred.log[[i]] <- predict(fit.log[[i]], newdata=validcumulative[,-1])
  print(summary(pred.log[[i]]))
  misclass[i]=mean(pred.log[[i]]!=validcumulative$group)
  
  #log-loss
  pred.logprobs[[i]] <-predict(fit.log[[i]], validcumulative[,-1], 'probs')
  logloss[i]=MultiLogLoss(y_pred = pred.logprobs[[i]], y_true = validcumulative$group) 
  
}
logloss
misclass

coef(fit.log[[3]])

```
```{r eval=FALSE, echo=FALSE}
mean(as.character(pred_log) != as.character(valid_log$group))
table(pred_log, valid_log$group)
```

### Performance
```{r results='asis'}
performancecategnb <-data.frame('features' = c('App Ct Binned','Prev +Is_Active Binned','Prev + Weekday Binned','Prev+hourly Binned', 'Prev + Day Binned'), 'Logloss' = c(2.884691,3.350532, 3.428049, 3.726826, 4.190035), 'Misclass' = c('84.61%', '84.46%', '84.50%', '84.61%', '84.42%'))
kable(performancecategnb, caption = 'Performance of Naive Bayes -- Binned Variables') %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "responsive", "condensed"), full_width = F)
```


```{r}
pbinned=list()
confusionbinned <- as.data.frame(table(pred.nb[[6]], validtotal$group))
pbinned[[5]]<-ggplot(confusionbinned, aes(Var2, Var1)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='Binned')
```







```{r results='asis'}
performancecategrf <-data.frame('features' = c('App Ct categorical features','Prev +Is_Active Categ','Prev + Weekday','Prev+hourly', 'Prev + Day'),'Logloss' = c(3.023014,	2.505135,2.489358,2.455428,2.347655), 'Misclass' = c(0.8295723,0.8254889,0.8259188,0.8235547,0.8183967))
kable(performancecategrf, caption = 'Performance of Random Forest with Categorical Variables') %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "responsive", "condensed"), full_width = F)
```

```{r results='asis'}
performancecategsvm <-data.frame('features' = c('App Ct categorical features','Prev +Is_Active Categ','Prev + Weekday','Prev+hourly', 'Prev + Day'),'Logloss' = c(2.655776,	2.671349,2.668522,2.668087,2.663947), 'Misclass' = c(0.8269933,0.8272083,0.8278530,0.8259188,0.8248442))
kable(performancecategsvm, caption = 'Performance of SVM with Categorical Variables') %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "responsive", "condensed"), full_width = F)
```

```{r results='asis'}
performancecateglog <-data.frame('features' = c('App Ct categorical features','Prev +Is_Active Categ','Prev + Weekday','Prev+hourly', 'Prev + Day'),'Logloss' = c(2.300363,	2.297325,2.297576,2.304471,2.299288), 'Misclass' = c(0.8151730,0.8149581,0.8160327,0.8076510,0.8076510))
kable(performancecateglog, caption = 'Performance of Logistic with Categorical Variables') %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "responsive", "condensed"), full_width = F)
```

### Scatter Plot of some combinations of count features
```{r fig.width=12, fig.height=38}
traincont <- as.data.frame(cbind(train.NoNANoM[,-c(1,3,4,66,67)], train.final[,c(3,4)]))
validcont <- as.data.frame(cbind(valid.NoNANoM[,-c(1,3,4,66,67)], valid.final[,c(3,4)]))
names <- names(traincont)
p<-list()

for (i in 2:63) {
  axis_x=traincont[,i]
  axis_y=traincont[,i+1]
  p[[names[i]]]<-ggplot(data = traincont) +
  geom_point(aes(x=axis_x, y = axis_y, color=group)) +
  xlim(range(axis_x)) + ylim(range(axis_y)) +
  labs(x = names[i], y = names[i+1])

}

marrangeGrob(p, ncol=2, nrow = 15)
```

There doesn't seem to be a clear distinction between groups in any of the plots.


# Same scatter plots, except the groups have been condensed from twelve to three classes
```{r fig.width=12, fig.height=38} 
# seperate into three groups
# M32-38, M39+ and other
training <- train.NoNANoM
validation <- valid.NoNANoM
train.group <- training$group
train.group[train.group!='M32-38' & train.group!='M39+'] <- 'other'
training$group <- train.group

training <- as.data.frame(training)
names <- names(training)
p<- list()

for (i in 3:64) {
  axis_x=training[,i]
  axis_y=training[,i+1]
  p[[names[i]]]<-ggplot(data = training) +
    geom_point(aes(x = axis_x, y = axis_y, color=group)) +
    xlim(range(axis_x)) + ylim(range(axis_y)) +
    labs(x = names[i], y = names[i+1]) 
}
marrangeGrob(p, ncol=2, nrow = 15)
```
The two commonly predicted classes, M32-38 and M39+, are clearly overlapping with the rest of the points.


# Female vs Male scatter plots
```{r fig.width=12, fig.height=38}
# plot F vs M groups
training <- train.NoNANoM
validation <- valid.NoNANoM
group <- training$group
gender <- substr(group, 1, 1)
training$group <- gender
training <- as.data.frame(training)
p <- list()

names <- names(training)

for (i in 3:64) {
  axis_x=training[,i]
  axis_y=training[,i+1]
  p[[names[i]]]<-ggplot(data = training) +
    geom_point(aes(x=axis_x, y = axis_y, color=group)) +
    xlim(range(axis_x)) + ylim(range(axis_y)) +
    labs(x = names[i], y = names[i+1]) 
}

marrangeGrob(p, ncol=2, nrow = 15)
```

## Stacked bar chart of the categorical features
```{r}
traincategdat<-cbind(trainingAppct, trainingIsactive, trainingWeekday, traincount, train.daycat)
traincategdat <- traincategdat[,-c(48:52)]
traincategdat <-traincategdat[,c(24,1:23,25:55)]
traincategdat$group <- as.factor(traincategdat$group)
p <-list()
names<-names(traincategdat)
```
```{r fig.width=12, fig.height=39}
for (i in 2:55) {
  temp <- traincategdat %>% 
  group_by(group, traincategdat[,i]) %>%
  summarise(count=n()) %>% 
  group_by(group) %>% 
  mutate(totals=sum(count)) %>% 
  mutate(proportion=count/totals)
  names(temp)[2] <- 'categ'
  
  p[[names[i]]]<-ggplot(data = temp) +
  geom_bar(aes(x=group,y=proportion, fill=categ), stat='identity')+
    labs(title=names[i])
}

marrangeGrob(p, ncol=2, nrow = 15)
```


## Remove features with low gini
```{r}
train_mod<-traintotal[,-'region.appCt']
train_mod<-train_mod[,-'sport.isactiveRat']
train_mod<-train_mod[,-'sport.appCt']
train_mod<-train_mod[,-'food.isactiveRat']
train_mod<-train_mod[,-'food.appCt']
train_mod<-train_mod[,-'Apr30']
train_mod<-train_mod[,-'estate.isactiveRat']
train_mod<-train_mod[,-'household.isactiveRat']
train_mod<-train_mod[,-'vehicle.isactiveRat']
train_mod<-train_mod[,-'education.isactiveRat']
train_mod<-train_mod[,-'estate.appCt']
train_mod<-train_mod[,-'household.appCt']
train_mod<-train_mod[,-'payment.isactiveRat']
train_mod<-train_mod[,-'game.isactiveRat']
train_mod<-train_mod[,-'photography.isactiveRat']
train_mod<-train_mod[,-'health.isactiveRat']
train_mod<-train_mod[,-'book.isactiveRat']
train_mod<-train_mod[,-'travel.isactiveRat']
train_mod<-train_mod[,-'family.isactiveRat']
train_mod<-train_mod[,-'May04']
train_mod<-train_mod[,-'May06']
train_mod<-train_mod[,-'May02']
train_mod<-train_mod[,-'May03']
train_mod<-train_mod[,-'entertainment.isactiveRat']
train_mod<-train_mod[,-'May07']
train_mod<-train_mod[,-'May05']
valid_mod<-validtotal[,-'region.appCt']
valid_mod<-valid_mod[,-'sport.isactiveRat']
valid_mod<-valid_mod[,-'sport.appCt']
valid_mod<-valid_mod[,-'food.isactiveRat']
valid_mod<-valid_mod[,-'food.appCt']
valid_mod<-valid_mod[,-'Apr30']
valid_mod<-valid_mod[,-'estate.isactiveRat']
valid_mod<-valid_mod[,-'household.isactiveRat']
valid_mod<-valid_mod[,-'vehicle.isactiveRat']
valid_mod<-valid_mod[,-'education.isactiveRat']
valid_mod<-valid_mod[,-'estate.appCt']
valid_mod<-valid_mod[,-'household.appCt']
valid_mod<-valid_mod[,-'payment.isactiveRat']
valid_mod<-valid_mod[,-'game.isactiveRat']
valid_mod<-valid_mod[,-'photography.isactiveRat']
valid_mod<-valid_mod[,-'health.isactiveRat']
valid_mod<-valid_mod[,-'book.isactiveRat']
valid_mod<-valid_mod[,-'travel.isactiveRat']
valid_mod<-valid_mod[,-'family.isactiveRat']
valid_mod<-valid_mod[,-'May04']
valid_mod<-valid_mod[,-'May06']
valid_mod<-valid_mod[,-'May02']
valid_mod<-valid_mod[,-'May03']
valid_mod<-valid_mod[,-'entertainment.isactiveRat']
valid_mod<-valid_mod[,-'May07']
valid_mod<-valid_mod[,-'May05']
```

### Logistic
```{r}
fit.log = list()
pred.log = list()
pred.logprobs = list()

fit.log <-  multinom(group ~ ., data = train_mod, MaxNWts = 100000, model = TRUE, probability = TRUE)
pred.log <- predict(fit.log, newdata=valid_mod)
summary(pred.log)
misclass=mean(pred.log!=valid_log$group)

#log-loss
pred.logprobs <-predict(fit.log, valid_log, 'probs')
logloss=MultiLogLoss(y_pred = pred.logprobs, y_true = valid_log$group) 

logloss
misclass

coef(fit.log[[3]])

```

### Naive Bayes
```{r}
fit.nb = list()
pred.nb = list()
pred.nbprobs = list()
logloss = vector(length=5)
misclass<-vector(length=5)

# traincumulative = matrix(nrow = dim(train.NoNANoM)[1])
# validcumulative = matrix(nrow = dim(valid.NoNANoM)[1])
traincumulative = matrix(nrow=18594)
validcumulative = matrix(nrow=4653)

for (i in 1:5) {
  traincumulative <- cbind(traincumulative, traincateg[[i]])
  validcumulative <- cbind(validcumulative, validcateg[[i]])
  fit.nb[[i]]<-naiveBayes(group~., data=traincumulative[,-1])
  pred.nb[[i]]<-predict(fit.nb[[i]], validcumulative[,-1])
  print(summary(pred.nb[[i]]))
  misclass[i]=mean(pred.nb[[i]]!=validcumulative$group)
  
  #log-loss
  pred.nbprobs[[i]] <-predict(fit.nb[[i]], validcumulative[,-1], type = 'raw')
  logloss[i]=MultiLogLoss(y_pred = pred.nbprobs[[i]], y_true = validcumulative$group) 
  
}
logloss
misclass
```

### RF
```{r}
set.seed(1223)
fit.rf = list()
pred.rf = list()
pred.rfprobs = list()
logloss = vector(length=5)
misclass<-vector(length=5)
traincumulative = matrix(nrow=18594)
validcumulative = matrix(nrow=4653)

for (i in 1:5) {
  traincumulative <- cbind(traincumulative, traincateg[[i]])
  validcumulative <- cbind(validcumulative, validcateg[[i]])
  fit.rf[[i]] <- randomForest(as.factor(group)~., data = traincumulative[,-1], na.action = na.omit)
  pred.rf[[i]] <- predict(fit.rf[[i]], newdata=validcumulative[,-1])
  print(summary(pred.rf[[i]]))
  misclass[i]=mean(pred.rf[[i]]!=validcumulative$group)
  
  #log-loss
  pred.rfprobs[[i]] <-predict(fit.rf[[i]], validcumulative[,-1], type = 'prob')
  logloss[i]=MultiLogLoss(y_pred = pred.rfprobs[[i]], y_true = validcumulative$group) 
  
}
misclass
logloss
```
```{r eval=FALSE,echo=FALSE}

table(rf.pred, validcateg$group)
mean(rf.pred[is.na(rf.pred)==FALSE]!=validcateg$group[is.na(rf.pred)==FALSE])




ginis<-importance(fit.rf)
ginis<-data.frame('gini'=ginis[,1], 'categ' = rownames(ginis))
arrange(ginis, desc(gini))
```

### SVM
```{r eval=FALSE}
fit.svm <- svm(as.factor(group) ~ . , train_mod, probability=TRUE)
pred.svm <- predict(fit.svm, newdata=valid_mod)
print(summary(pred.svm))
misclass=mean(pred.svm!=valid_mod$group)

#log-loss
pred.svmprobs <-predict(fit.svm, valid_mod, probability = T)
pred.svmprobs <- attr(pred.svmprobs, 'probabilities')
logloss=MultiLogLoss(y_pred = pred.svmprobs, y_true = valid_mod$group) 
logloss
misclass
```

```{r results='asis'}
performancesvmred <-data.frame('Misclassification' = c("81.78%"), 'Logloss' = c(2.661075))
kable(performancesvmred, caption = "SVM Performance: Reduced Predictors") %>%
  kable_styling(bootstrap_options = c("striped", "responsive", "condensed"), full_width = F)
```
```{r}
confusionsvmred <- as.data.frame(table(pred.svm, validtotal$group))
psvmred<-ggplot(confusionsvmred, aes(Var2, pred.svm)) +
  geom_tile(aes(fill = Freq), colour='gray') +
  scale_fill_gradient(low='antiquewhite', high = 'purple') +
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 12)) +
  labs(x= 'Observed', y= 'Predicted', title = 'Confusion Matrix', subtitle='Reduced Features')
```